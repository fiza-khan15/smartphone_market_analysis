{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000811fb-2aba-438f-92bb-6af05a973c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import re # Added for extracting RAM and 5G info\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "355a7353-1a26-49d5-9d26-ae08fd9c5921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "TOKEN = \"api_token\" # <--- PASTE YOUR TOKEN HERE. I use Scrape.do api key\n",
    "PAGES_TO_SCRAPE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deda4f59-a99f-4707-b648-0b92e01c4854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Scraping Process ---\n",
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Fetching page 5...\n",
      "Fetching page 6...\n",
      "Fetching page 7...\n",
      "Fetching page 8...\n",
      "Fetching page 9...\n",
      "Fetching page 10...\n",
      "\n",
      "Successfully collected 10 HTML pages.\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 1: SCRAPING ---\n",
    "print(\"--- Starting Scraping Process ---\")\n",
    "all_html_pages = [] \n",
    "\n",
    "for page_num in range(1, PAGES_TO_SCRAPE + 1):\n",
    "    print(f\"Fetching page {page_num}...\")\n",
    "    \n",
    "    # 1. Define the target URL (Flipkart search for smartphones)\n",
    "    target_url = f\"https://www.add_page_url=smartphones&page={page_num}\"\n",
    "    \n",
    "    # 2. Encode the URL for the API\n",
    "    encoded_url = urllib.parse.quote(target_url)\n",
    "    \n",
    "    # 3. Construct Scrap.do API URL\n",
    "    api_url = f\"http://api.scrape.do/?token={TOKEN}&url={encoded_url}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        if response.status_code == 200:\n",
    "            all_html_pages.append(response.text)\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {page_num}. Status Code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching page {page_num}: {e}\")\n",
    "    \n",
    "    # Optional: Sleep briefly to be polite/avoid errors\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\nSuccessfully collected {len(all_html_pages)} HTML pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fafde52e-64ee-4378-a65f-0afd691bd3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Parsing Data ---\n",
      "Extracted 240 products.\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 2: PARSING ---\n",
    "def parse_html_pages(html_list):\n",
    "    all_products = []\n",
    "    \n",
    "    for html_content in html_list:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Flipkart product card class\n",
    "        product_cards = soup.find_all('div', {'class': 'cPHDOP'})\n",
    "        \n",
    "        for card in product_cards:\n",
    "            try:\n",
    "                # Extract raw details\n",
    "                name_el = card.find('div', {'class': 'KzDlHZ'})\n",
    "                price_el = card.find('div', {'class': 'Nx9bqj _4b5DiR'})\n",
    "                rating_el = card.find('div', {'class': 'XQDdHH'})\n",
    "                \n",
    "                if name_el and price_el:\n",
    "                    name_text = name_el.text.strip()\n",
    "                    price_text = price_el.text.strip()\n",
    "                    rating_text = rating_el.text.strip() if rating_el else \"0\"\n",
    "                    \n",
    "                    all_products.append({\n",
    "                        \"Product Name\": name_text,\n",
    "                        \"Original Price\": price_text,\n",
    "                        \"Rating\": rating_text\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "    return all_products\n",
    "\n",
    "print(\"--- Parsing Data ---\")\n",
    "raw_data = parse_html_pages(all_html_pages)\n",
    "print(f\"Extracted {len(raw_data)} products.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ae8e8cf-4358-4216-be94-de5eebf3f1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Product Name': 'POCO C71 (Cool Blue, 128 GB)', 'Original Price': '₹6,799', 'Rating': '4.1'}, {'Product Name': 'Ai+ Pulse (Blue, 64 GB)', 'Original Price': '₹5,999', 'Rating': '4.3'}, {'Product Name': 'realme P3 Lite 5G Charger in the Box (Midnight Lily, 128 GB)', 'Original Price': '₹9,749', 'Rating': '4.4'}]\n"
     ]
    }
   ],
   "source": [
    "print(raw_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62217bb8-2db6-40e6-9759-831af9973cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to my_products.csv using Pandas!\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a DataFrame from your data list\n",
    "df = pd.DataFrame(raw_data)\n",
    "\n",
    "# 2. Save it to a CSV file\n",
    "# index=False prevents pandas from adding an extra row of numbers (0, 1, 2...)\n",
    "df.to_csv('my_products.csv', index=False)\n",
    "\n",
    "print(\"Saved to my_products.csv using Pandas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d430e39-bca2-4381-a001-acf192753dac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
